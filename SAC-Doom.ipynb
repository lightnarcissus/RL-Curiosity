{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import tensorflow.contrib.slim as slim\n",
    "import scipy.signal\n",
    "%matplotlib inline\n",
    "from helper import *\n",
    "from vizdoom import *\n",
    "\n",
    "from random import choice\n",
    "from time import sleep\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copies one set of variables to another.\n",
    "# Used to set worker network parameters to those of global network.\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "# Processes Doom screen image to produce cropped and resized image. \n",
    "def process_frame(frame):\n",
    "    s = frame[10:-10,30:-30]\n",
    "    s = scipy.misc.imresize(s,[84,84])\n",
    "    s = np.reshape(s,[np.prod(s.shape)]) / 255.0\n",
    "    return s\n",
    "\n",
    "# Discounting function used to calculate discounted returns.\n",
    "def discount(x, gamma):\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "#Used to initialize weights for policy and value output layers\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self,\n",
    "                 actor,\n",
    "                 critic,\n",
    "                 value,\n",
    "                 obs_dim,\n",
    "                 num_actions,\n",
    "                 replay_buffer,\n",
    "                 batch_size=4,\n",
    "                 action_scale=2.0,\n",
    "                 gamma=0.9,\n",
    "                 tau=0.01,\n",
    "                 actor_lr=3*1e-3,\n",
    "                 critic_lr=3*1e-3,\n",
    "                 value_lr=3*1e-3,\n",
    "                 reg_factor=1e-3):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_scale = action_scale\n",
    "        self.last_obs = None\n",
    "        self.t = 0\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "        self._act,\\\n",
    "        self._train_actor,\\\n",
    "        self._train_critic,\\\n",
    "        self._train_value,\\\n",
    "        self._update_target = build_graph(\n",
    "            actor=actor,\n",
    "            critic=critic,\n",
    "            value=value,\n",
    "            obs_dim=obs_dim,\n",
    "            num_actions=num_actions,\n",
    "            batch_size=batch_size,\n",
    "            gamma=gamma,\n",
    "            tau=tau,\n",
    "            actor_lr=actor_lr,\n",
    "            critic_lr=critic_lr,\n",
    "            value_lr=value_lr,\n",
    "            reg_factor=reg_factor\n",
    "        )\n",
    "\n",
    "        self.actor_errors = []\n",
    "        self.critic_errors = []\n",
    "        self.value_errors = []\n",
    "\n",
    "    def act(self, obs, reward, training=True):\n",
    "        obs = obs[0]\n",
    "        action, greedy_action = np.clip(self._act([obs]), -1, 1)\n",
    "        action = action[0]\n",
    "        greedy_action = greedy_action[0]\n",
    "\n",
    "        if not training:\n",
    "            action = greedy_action\n",
    "\n",
    "        if training and self.t > 10 * 200:\n",
    "            # sample experiences\n",
    "            obs_t,\\\n",
    "            actions,\\\n",
    "            rewards,\\\n",
    "            obs_tp1,\\\n",
    "            dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "            # update networks\n",
    "            value_error = self._train_value(obs_t, actions)\n",
    "            critic_error = self._train_critic(\n",
    "                obs_t, actions, rewards, obs_tp1, dones)\n",
    "            actor_error = self._train_actor(obs_t, actions)\n",
    "\n",
    "            # store errors through episode\n",
    "            self.value_errors.append(value_error)\n",
    "            self.critic_errors.append(critic_error)\n",
    "            self.actor_errors.append(actor_error)\n",
    "\n",
    "            # update target networks\n",
    "            self._update_target()\n",
    "\n",
    "        if training and self.last_obs is not None:\n",
    "            self.replay_buffer.append(\n",
    "                obs_t=self.last_obs,\n",
    "                action=self.last_action,\n",
    "                reward=reward,\n",
    "                obs_tp1=obs,\n",
    "                done=False\n",
    "            )\n",
    "\n",
    "        self.t += 1\n",
    "        self.last_obs = obs\n",
    "        self.last_action = action\n",
    "        return action * self.action_scale\n",
    "\n",
    "    def stop_episode(self, obs, reward, training=True):\n",
    "        obs = obs[0]\n",
    "        if training:\n",
    "            self.replay_buffer.append(\n",
    "                obs_t=self.last_obs,\n",
    "                action=self.last_action,\n",
    "                reward=reward,\n",
    "                obs_tp1=obs,\n",
    "                done=True\n",
    "            )\n",
    "            print('actor error: {}, critic error: {}, value error: {}'.format(\n",
    "                sum(self.actor_errors), sum(self.critic_errors), sum(self.value_errors)))\n",
    "        self.last_obs = None\n",
    "        self.last_action = []\n",
    "        self.value_errors = []\n",
    "        self.critic_errors = []\n",
    "        self.actor_errors = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build SAC Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC_Network():\n",
    "    def __init__(self,hiddens,inpt,num_actions,initializer=tf.contrib.layers.xavier_initializer(),reg_factor=1e-3):\n",
    "        #make actor network\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            # l2 regularizer\n",
    "            regularizer = layers.l2_regularizer(scale=reg_factor)\n",
    "            out = inpt\n",
    "            for hidden in hiddens:\n",
    "                out = tf.layers.dense(\n",
    "                    out, hidden,\n",
    "                    bias_initializer=tf.constant_initializer(0.0),\n",
    "                    kernel_initializer=initializer,\n",
    "                    kernel_regularizer=regularizer)\n",
    "                out = tf.nn.relu(out)\n",
    "\n",
    "            # mean value of normal distribution\n",
    "            mu = tf.layers.dense(\n",
    "                out, num_actions, kernel_initializer=initializer, name='mu')\n",
    "            greedy_action = tf.nn.tanh(mu)\n",
    "\n",
    "            # variance of normal distribution\n",
    "            sigma = tf.layers.dense(\n",
    "                out, num_actions, kernel_initializer=initializer, name='sigma')\n",
    "\n",
    "            # sample actions from normal distribution\n",
    "            dist = tf.distributions.Normal(mu, tf.exp(sigma))\n",
    "            out = tf.reshape(dist.sample(num_actions), [-1, num_actions])\n",
    "            out = tf.stop_gradient(out)\n",
    "            action = tf.nn.tanh(out)\n",
    "            log_prob = dist.log_prob(out) - tf.log(1 - action ** 2 + 1e-6)\n",
    "        self.action= action \n",
    "        self.greedy_action = greedy_action \n",
    "        self.log_prob = log_prob\n",
    "        self.regularizer=regularizer\n",
    "        \n",
    "        #make critic network\n",
    "        out = tf.concat([inpt, action], axis=3)\n",
    "        for hidden in hiddens:\n",
    "            out = tf.layers.dense(out, hidden,\n",
    "                                  bias_initializer=tf.constant_initializer(0.0),\n",
    "                                  kernel_initializer=initializer)\n",
    "            out = tf.nn.relu(out)\n",
    "        out = tf.layers.dense(out, 1, kernel_initializer=initializer)\n",
    "        self.critic_network=out\n",
    "        #make value network\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            out = inpt\n",
    "            for hidden in hiddens:\n",
    "                out = tf.layers.dense(\n",
    "                    out, hidden,\n",
    "                    bias_initializer=tf.constant_initializer(0.0),\n",
    "                    kernel_initializer=initializer)\n",
    "                out = tf.nn.relu(out)\n",
    "\n",
    "            out = tf.layers.dense(out, 1, kernel_initializer=initializer)\n",
    "        self.value_network=out\n",
    "            \n",
    "\n",
    "# def make_actor_network(hiddens):\n",
    "#     return lambda *args, **kwargs: _make_actor_network(hiddens, *args, **kwargs)\n",
    "\n",
    "# def make_critic_network(hiddens):\n",
    "#     return lambda *args, **kwargs: _make_critic_network(hiddens, *args, **kwargs)\n",
    "\n",
    "# def make_value_network(hiddens):\n",
    "#     return lambda *args, **kwargs: _make_value_network(hiddens, *args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build graph for updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(actor,\n",
    "                critic,\n",
    "                value,\n",
    "                obs_dim,\n",
    "                num_actions,\n",
    "                batch_size,\n",
    "                gamma,\n",
    "                tau,\n",
    "                actor_lr,\n",
    "                critic_lr,\n",
    "                value_lr,\n",
    "                reg_factor,\n",
    "                scope='sac',\n",
    "                reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # input placeholders\n",
    "        print(\"OBS DIM in graph \" + str(obs_dim))\n",
    "        obs_t_input = tf.placeholder(tf.float32, shape=[None,obs_dim[0],obs_dim[1],obs_dim[2]], name='obs_t')\n",
    "        act_t_ph = tf.placeholder(tf.float32, [None, num_actions], name='action')\n",
    "        rew_t_ph = tf.placeholder(tf.float32, [None], name='reward')\n",
    "        obs_tp1_input = tf.placeholder(tf.float32, shape=[None,obs_dim[0],obs_dim[1],obs_dim[2]], name='obs_tp1')\n",
    "        done_mask_ph = tf.placeholder(tf.float32, [None], name='done')\n",
    "        print(\"obs_t_input \" + str(obs_t_input))\n",
    "        # actor network\n",
    "        policy_t, greedy_policy_t, log_pi_t, reg = actor(\n",
    "            obs_t_input, num_actions, reg_factor=reg_factor, scope='actor')\n",
    "        actor_func_vars = tf.get_collection(\n",
    "            tf.GraphKeys.TRAINABLE_VARIABLES, '{}/actor'.format(scope))\n",
    "        print(\"finished setting actor network\")\n",
    "        # critic network\n",
    "        q_t = critic(obs_t_input, act_t_ph, scope='critic')\n",
    "        q_t_with_actor = critic(\n",
    "            obs_t_input, policy_t, scope='critic', reuse=True)\n",
    "        critic_func_vars = tf.get_collection(\n",
    "            tf.GraphKeys.TRAINABLE_VARIABLES, '{}/critic'.format(scope))\n",
    "        print(\"finished setting critic network\")\n",
    "        # value network\n",
    "        v_t = value(obs_t_input, scope='value')\n",
    "        value_func_vars = tf.get_collection(\n",
    "            tf.GraphKeys.TRAINABLE_VARIABLES, '{}/value'.format(scope))\n",
    "        print(\"finished setting value network\")\n",
    "        # target value network\n",
    "        v_tp1 = value(obs_tp1_input, scope='target_value')\n",
    "        target_func_vars = tf.get_collection(\n",
    "            tf.GraphKeys.TRAINABLE_VARIABLES, '{}/target_value'.format(scope))\n",
    "        print(\"finished setting target value network\")\n",
    "        with tf.variable_scope('value_loss'):\n",
    "            target = q_t - log_pi_t\n",
    "            value_loss = tf.reduce_mean(\n",
    "                0.5 * tf.square(v_t - tf.stop_gradient(target)))\n",
    "\n",
    "        with tf.variable_scope('critic_loss'):\n",
    "            target = rew_t_ph + gamma * v_tp1 * (1.0 - done_mask_ph)\n",
    "            critic_loss = tf.reduce_mean(\n",
    "                0.5 * tf.square(q_t - tf.stop_gradient(target)))\n",
    "\n",
    "        with tf.variable_scope('policy_loss'):\n",
    "            target = q_t_with_actor - v_t\n",
    "            actor_loss = 0.5 * tf.reduce_mean(\n",
    "                log_pi_t * tf.stop_gradient(log_pi_t - target))\n",
    "            reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "            l2_loss = layers.apply_regularization(reg, reg_variables)\n",
    "            actor_loss = actor_loss + l2_loss\n",
    "\n",
    "        # optimize operations\n",
    "        critic_optimizer = tf.train.AdamOptimizer(critic_lr)\n",
    "        critic_optimize_expr = critic_optimizer.minimize(\n",
    "            critic_loss, var_list=critic_func_vars)\n",
    "        actor_optimizer = tf.train.AdamOptimizer(actor_lr)\n",
    "        actor_optimize_expr = actor_optimizer.minimize(\n",
    "            actor_loss, var_list=actor_func_vars)\n",
    "        value_optimizer = tf.train.AdamOptimizer(value_lr)\n",
    "        value_optimize_expr = value_optimizer.minimize(\n",
    "            value_loss, var_list=value_func_vars)\n",
    "\n",
    "        # update critic target operations\n",
    "        with tf.variable_scope('update_value_target'):\n",
    "            update_target_expr = []\n",
    "            sorted_vars = sorted(value_func_vars, key=lambda v: v.name)\n",
    "            sorted_target_vars = sorted(target_func_vars, key=lambda v: v.name)\n",
    "            # assign value variables to target value variables\n",
    "            for var, var_target in zip(sorted_vars, sorted_target_vars):\n",
    "                new_var = tau * var + (1 - tau) * var_target\n",
    "                update_target_expr.append(var_target.assign(new_var))\n",
    "            update_target_expr = tf.group(*update_target_expr)\n",
    "\n",
    "        def act(obs):\n",
    "            feed_dict = {\n",
    "                obs_t_input: obs\n",
    "            }\n",
    "            print(\"policy t \" + str(policy_t))\n",
    "            print(\"greedy policy \"  + str(greedy_policy_t))\n",
    "            # feed_dict=tf.reshape(feed_dict,[1,240,320,3])\n",
    "            print(\"feed_dict \"  + str(feed_dict))\n",
    "            return tf.get_default_session().run(\n",
    "                [policy_t, greedy_policy_t], feed_dict=feed_dict)\n",
    "\n",
    "        def train_actor(obs, action):\n",
    "            feed_dict = {\n",
    "                obs_t_input: obs,\n",
    "                act_t_ph: action\n",
    "            }\n",
    "            loss_val, _ = tf.get_default_session().run(\n",
    "                [actor_loss, actor_optimize_expr], feed_dict=feed_dict)\n",
    "            return loss_val\n",
    "\n",
    "        def train_critic(obs_t, action, rew, obs_tp1, done):\n",
    "            feed_dict = {\n",
    "                obs_t_input: obs_t,\n",
    "                act_t_ph: action,\n",
    "                rew_t_ph: rew,\n",
    "                obs_tp1_input: obs_tp1,\n",
    "                done_mask_ph: done\n",
    "            }\n",
    "            loss_val, _ = tf.get_default_session().run(\n",
    "                [critic_loss, critic_optimize_expr], feed_dict=feed_dict)\n",
    "            return loss_val\n",
    "\n",
    "        def train_value(obs_t, action):\n",
    "            feed_dict = {\n",
    "                obs_t_input: obs_t,\n",
    "                act_t_ph: action\n",
    "            }\n",
    "            loss_val, _ = tf.get_default_session().run(\n",
    "                [value_loss, value_optimize_expr], feed_dict=feed_dict)\n",
    "            return loss_val\n",
    "\n",
    "        def update_target():\n",
    "            tf.get_default_session().run(update_target_expr)\n",
    "\n",
    "        return act, train_actor, train_critic, train_value, update_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self,game,name,s_size,a_size,trainer,model_path,global_episodes):\n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name        \n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.summary_writer = tf.summary.FileWriter(\"train_\"+str(self.number))\n",
    "        hiddens=[128,128]\n",
    "        #Create the local copy of the network and the tensorflow op to copy global paramters to local network\n",
    "        self.local_SAC = SAC_Network(s_size,a_size,self.name,trainer)\n",
    "        self.update_local_ops = update_target_graph('global',self.name)        \n",
    "        \n",
    "        #The Below code is related to setting up the Doom environment\n",
    "        game.set_doom_scenario_path(\"scenarios/health_gathering_supreme.wad\") #This corresponds to the simple task we will pose our agent\n",
    "        game.set_doom_map(\"map01\")\n",
    "        game.set_screen_resolution(ScreenResolution.RES_160X120)\n",
    "        game.set_screen_format(ScreenFormat.GRAY8)\n",
    "        game.set_render_hud(False)\n",
    "        game.set_render_crosshair(False)\n",
    "        game.set_render_weapon(True)\n",
    "        game.set_render_decals(False)\n",
    "        game.set_render_particles(False)\n",
    "        game.add_available_button(Button.MOVE_LEFT)\n",
    "        game.add_available_button(Button.MOVE_RIGHT)\n",
    "        game.add_available_button(Button.ATTACK)\n",
    "        game.add_available_game_variable(GameVariable.AMMO2)\n",
    "        game.add_available_game_variable(GameVariable.POSITION_X)\n",
    "        game.add_available_game_variable(GameVariable.POSITION_Y)\n",
    "        game.set_episode_timeout(300)\n",
    "        game.set_episode_start_time(10)\n",
    "        game.set_window_visible(False)\n",
    "        game.set_sound_enabled(False)\n",
    "        game.set_living_reward(-1)\n",
    "        game.set_mode(Mode.PLAYER)\n",
    "        game.init()\n",
    "        self.actions = self.actions = np.identity(a_size,dtype=bool).tolist()\n",
    "        #End Doom set-up\n",
    "        self.env = game\n",
    "        \n",
    "    def train(self,rollout,sess,gamma,bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        observations = rollout[:,0]\n",
    "        actions = rollout[:,1]\n",
    "        rewards = rollout[:,2]\n",
    "        next_observations = rollout[:,3]\n",
    "        values = rollout[:,5]\n",
    "        \n",
    "        # Here we take the rewards and values from the rollout, and use them to \n",
    "        # generate the advantage and discounted returns. \n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus,gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        advantages = discount(advantages,gamma)\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        feed_dict = {self.local_SAC.target_v:discounted_rewards,\n",
    "            self.local_SAC.inputs:np.vstack(observations),\n",
    "            self.local_SAC.actions:actions,\n",
    "            self.local_SAC.advantages:advantages,\n",
    "            self.local_SAC.state_in[0]:self.batch_rnn_state[0],\n",
    "            self.local_SAC.state_in[1]:self.batch_rnn_state[1]}\n",
    "        v_l,p_l,e_l,g_n,v_n, self.batch_rnn_state,_ = sess.run([self.local_SAC.value_loss,\n",
    "            self.local_SAC.policy_loss,\n",
    "            self.local_SAC.entropy,\n",
    "            self.local_SAC.grad_norms,\n",
    "            self.local_SAC.var_norms,\n",
    "            self.local_SAC.state_out,\n",
    "            self.local_SAC.apply_grads],\n",
    "            feed_dict=feed_dict)\n",
    "        return v_l / len(rollout),p_l / len(rollout),e_l / len(rollout), g_n,v_n\n",
    "        \n",
    "    def work(self,max_episode_length,gamma,sess,coord,saver):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        total_steps = 0\n",
    "        print (\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():                 \n",
    "            while not coord.should_stop():\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_values = []\n",
    "                episode_frames = []\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "                d = False\n",
    "                \n",
    "                self.env.new_episode()\n",
    "                s = self.env.get_state().screen_buffer\n",
    "                episode_frames.append(s)\n",
    "                s = process_frame(s)\n",
    "                rnn_state = self.local_SAC.state_init\n",
    "                self.batch_rnn_state = rnn_state\n",
    "                while self.env.is_episode_finished() == False:\n",
    "                    #Take an action using probabilities from policy network output.\n",
    "                    a_dist,v,rnn_state = sess.run([self.local_SAC.policy,self.local_SAC.value,self.local_SAC.state_out], \n",
    "                        feed_dict={self.local_AC.inputs:[s],\n",
    "                        self.local_SAC.state_in[0]:rnn_state[0],\n",
    "                        self.local_SAC.state_in[1]:rnn_state[1]})\n",
    "                    a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                    a = np.argmax(a_dist == a)\n",
    "\n",
    "                    r = self.env.make_action(self.actions[a]) / 100.0\n",
    "                    d = self.env.is_episode_finished()\n",
    "                    if d == False:\n",
    "                        s1 = self.env.get_state().screen_buffer\n",
    "                        episode_frames.append(s1)\n",
    "                        s1 = process_frame(s1)\n",
    "                    else:\n",
    "                        s1 = s\n",
    "                        \n",
    "                    episode_buffer.append([s,a,r,s1,d,v[0,0]])\n",
    "                    episode_values.append(v[0,0])\n",
    "\n",
    "                    episode_reward += r\n",
    "                    s = s1                    \n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "                    \n",
    "                    # If the episode hasn't ended, but the experience buffer is full, then we\n",
    "                    # make an update step using that experience rollout.\n",
    "                    if len(episode_buffer) == 30 and d != True and episode_step_count != max_episode_length - 1:\n",
    "                        # Since we don't know what the true final return is, we \"bootstrap\" from our current\n",
    "                        # value estimation.\n",
    "                        v1 = sess.run(self.local_AC.value, \n",
    "                            feed_dict={self.local_AC.inputs:[s],\n",
    "                            self.local_AC.state_in[0]:rnn_state[0],\n",
    "                            self.local_AC.state_in[1]:rnn_state[1]})[0,0]\n",
    "                        v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,v1)\n",
    "                        episode_buffer = []\n",
    "                        sess.run(self.update_local_ops)\n",
    "                    if d == True:\n",
    "                        break\n",
    "                                            \n",
    "                self.episode_rewards.append(episode_reward)\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(np.mean(episode_values))\n",
    "                \n",
    "                # Update the network using the episode buffer at the end of the episode.\n",
    "                if len(episode_buffer) != 0:\n",
    "                    v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,0.0)\n",
    "                                \n",
    "                    \n",
    "                # Periodically save gifs of episodes, model parameters, and summary statistics.\n",
    "                if episode_count % 5 == 0 and episode_count != 0:\n",
    "                    if self.name == 'worker_0' and episode_count % 25 == 0:\n",
    "                        time_per_step = 0.05\n",
    "                        images = np.array(episode_frames)\n",
    "                        make_gif(images,'./frames/image'+str(episode_count)+'.gif',\n",
    "                            duration=len(images)*time_per_step,true_image=True,salience=False)\n",
    "                    if episode_count % 250 == 0 and self.name == 'worker_0':\n",
    "                        saver.save(sess,self.model_path+'/model-'+str(episode_count)+'.cptk')\n",
    "                        print (\"Saved Model\")\n",
    "\n",
    "                    mean_reward = np.mean(self.episode_rewards[-5:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-5:])\n",
    "                    mean_value = np.mean(self.episode_mean_values[-5:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Perf/Reward', simple_value=float(mean_reward))\n",
    "                    summary.value.add(tag='Perf/Length', simple_value=float(mean_length))\n",
    "                    summary.value.add(tag='Perf/Value', simple_value=float(mean_value))\n",
    "                    summary.value.add(tag='Losses/Value Loss', simple_value=float(v_l))\n",
    "                    summary.value.add(tag='Losses/Policy Loss', simple_value=float(p_l))\n",
    "                    summary.value.add(tag='Losses/Entropy', simple_value=float(e_l))\n",
    "                    summary.value.add(tag='Losses/Grad Norm', simple_value=float(g_n))\n",
    "                    summary.value.add(tag='Losses/Var Norm', simple_value=float(v_n))\n",
    "                    self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
